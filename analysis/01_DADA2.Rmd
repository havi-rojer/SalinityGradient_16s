---
title: "01_DADA2"
output: html_document
  toc: yes
  toc_float:
    collapse: no
    smooth_scroll: yes
    toc_depth: 3
date: "`r Sys.Date()`"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", fig.path = "../figures/01_DADA2/") # Send any figures to this folder
```

# Before you start

## Set my seed

```{r set-seed}
# Any number can be chosen
set.seed(3141596)
```

# Goals of this file

1. Use raw fastq files and generate quality plots to assess quality of reads.
2. Filter and trim out bad sequences and bases from our sequencing files.
3. Write out fastq files with high quality sequences.
4. Evaluate the quality from our filter and trim.
5. Infer errors on forward and reverse reads individually.
6. Identified ASVs on forward and reverse reads seperately using the error model.
7. Merge forward and reverse ASVs into "contiguous ASVs".
8. Generate the ASV count table (`otu_table` input for phyloseq).

Output that we need:

1. ASV Count Table: `otu_table`
2. Taxonomy Table: `tax_table`
3. Sample Information: `sample_data` - track the reads lost throughout the DADA2 workflow

# Load Libraries
```{r load-libraries}
#install.packages("devtools")
library(devtools)

#devtools::install_github("benjjneb/dada2")
library(dada2)

#install.packages("tidyverse")
library(tidyverse)

#install.packages("patchwork)
library(patchwork)

#install.packages("dplyr")
library(dplyr)
```

```{r load-data}
# Set the raw fastq paths to the raw sequencing file
# Path to the fastq files
setwd("/local/workdir/zlr6/git_repos/SalinityGradient_16s")
raw_fastqs_path <- "data/01_DADA2/01_raw_gzipped_fastqs"
raw_fastqs_path

# What files are in this path?
list.files(raw_fastqs_path)

# How many files are there?
str(list.files(raw_fastqs_path))

# Create vector of forward reads
forward_reads <- list.files(raw_fastqs_path, pattern = "R1_001.fastq.gz", full.names = TRUE)
# Intuition check
head(forward_reads)
 
# Create vector of reverse reads
reverse_reads <- list.files(raw_fastqs_path, pattern = "R2_001.fastq.gz", full.names = TRUE)
# Intuition check
head(reverse_reads)
```

# Raw Quality Plots
```{r raw-quality-plot}
# Randomly select 2 samples from dataset to evaluate
random_samples <- sample(1:length(reverse_reads), size = 2)
random_samples

# Calculate and plot quality of these two samples
plotQualityProfile(forward_reads[random_samples]) + 
  labs(title = "Forward Read Raw Quality")
plotQualityProfile(reverse_reads[random_samples]) + 
  labs(title = "Reverse Read Raw Quality")
```

# Prepare a placeholder for filtered reads
```{r prep-filtered-sequences}
# vector of our samples, extract sample name from files
samples <- sapply(strsplit(basename(forward_reads), "_"), `[`,1)

#Place filtered reads into filtered_fastqs_path
filtered_fastqs_path <- "data/01_DADA2/02_filtered_fastqs/"

# create 2 variables: filtered_F, filtered_R

filtered_forward_reads <- file.path(filtered_fastqs_path, paste0(samples, "_R1_filtered.fastq.gz"))

# Intuition check
length(filtered_forward_reads)

filtered_reverse_reads <- file.path(filtered_fastqs_path, paste0(samples, "_R2_filtered.fastq.gz"))

# Intuition check
head(filtered_reverse_reads)
```

# Filter and Trim Reads

Parameters of filter and trim **DEPENDS ON THE DATASET**

- `maxN` = number of N bases. Removes all Ns from the data.
- `maxEE` = Quality filtering threshold applied to expected errors. Here, if there's 2 expected errors, it's ok. But more than 2, throw away the sequence. Two values, first is for forward reads; second is for reverse reads.
- `trimLeft` = remove first three basepairs.
- `truncQ`

```{r filter-and-trim}
# Assign a vector to filtered reads
# trim out poor bases, first 3 bases on F reads
# write out filtered fastq files
filtered_reads <- filterAndTrim(fwd = forward_reads, filt = filtered_forward_reads, rev = reverse_reads, filt.rev = filtered_reverse_reads, maxN = 0, maxEE = (c(2,2)), trimLeft = 3, truncQ = 2, rm.phix = TRUE, compress = TRUE) # multithread = TRUE
```

# Plot the quality of the trimmed reads

```{r filterTrim-quality-plot}
# Calculate and plot quality of these two samples
plotQualityProfile(filtered_forward_reads[random_samples]) + 
  labs(title = "Trimmed Forward Read Quality")
plotQualityProfile(filtered_reverse_reads[random_samples]) + 
  labs(title = "Trimmed Reverse Read Quality")
```

# Aggregated Trimmed Plots
```{r aggregate-trimmed-plots}
# Aggregate all QC plots
# Install and library patchwork
# plotQualityProfile(filtered_forward_reads, aggregate = TRUE)
# plotQualityProfile(filtered_reverse_reads, aggregate = TRUE)
```

## Stats on read outputs from `filterAndTrim`
```{r filterTrim-stats}
# Make output into dataframe
filtered_df <- as.data.frame(filtered_reads)
head(filtered_df)

# calculate some stats
filtered_df %>%
  reframe(median_reads_in = median(reads.in),
          median_reads_out = median(reads.out),
          median_percent_retained = 100*median(reads.out)/median(reads.in))
```

# Error Modeling

**NOTE:** Run seperately one each illumina dataset.
```{r learn-errors}
# Forward reads
error_forward_Reads <- learnErrors(filtered_forward_reads) # multithread = TRUE
# Plot forward
plotErrors(error_forward_Reads, nominalQ = TRUE) +
  labs(title = "Forward Read Error Model")

# Reverse reads
error_reverse_Reads <- learnErrors(filtered_reverse_reads) # multithread = TRUE
# Plot reverse
plotErrors(error_reverse_Reads, nominalQ = TRUE) +
  labs(title = "Reverse Read Error Model")


```

# Infer ASVs

Note that this is happening seperately on the forward and reverse reads. This is unique to DADA2.

```{r infer-ASVs}
# Infer forward ASVs
dada_forward <- dada(filtered_forward_reads, err = error_forward_Reads) # multithread = TRUE

# Infer reverse ASVs
dada_reverse <- dada(filtered_reverse_reads, err = error_forward_Reads) # multithread = TRUE
```

# Merge forward and reverse ASVs
```{r merge-ASVs}
# merge forward and reverse ASVs
merged_ASVs <- mergePairs(dada_forward, filtered_forward_reads,
                          dada_reverse, filtered_reverse_reads, 
                          verbose = TRUE)

# Evaluate the output
typeof(merged_ASVs)
length(merged_ASVs)
names(merged_ASVs)
```

# Generate ASV Count Table
```{r generate-ASV-table}
# Create the ASV count table
raw_ASV_table <- makeSequenceTable(merged_ASVs)
```

# Session Information
```{r session-info}
# Ensure reproducibility
devtools::session_info()
```